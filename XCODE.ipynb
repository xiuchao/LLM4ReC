{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GroundingDINO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "(Pdb) model._modules.keys()\n",
    "['transformer', 'bert', 'feat_map', 'input_proj', 'backbone', 'bbox_embed', 'class_embed'])\n",
    "\n",
    "### \n",
    "(Pdb) model.transformer._modules.keys()\n",
    "['encoder', 'decoder', 'tgt_embed', 'enc_output', 'enc_output_norm', 'enc_out_bbox_embed', 'enc_out_class_embed']\n",
    "\n",
    "(Pdb) model.transformer.tgt_embed\n",
    "Embedding(900, 256)\n",
    "(Pdb) model.transformer.enc_output\n",
    "Linear(in_features=256, out_features=256, bias=True)\n",
    "(Pdb) model.transformer.enc_out_bbox_embed\n",
    "MLP((layers): ModuleList(\n",
    "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
    "    (1): Linear(in_features=256, out_features=256, bias=True)\n",
    "    (2): Linear(in_features=256, out_features=4, bias=True)))\n",
    "(Pdb) model.transformer.enc_out_class_embed\n",
    "ContrastiveEmbed()\n",
    "\n",
    "(Pdb) model.transformer.encoder\n",
    "TransformerEncoder(\n",
    "  (layers): ModuleList(\n",
    "    (0-5): DeformableTransformerEncoderLayer(\n",
    "      (self_attn): MultiScaleDeformableAttention(\n",
    "        (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
    "        (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
    "        (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
    "        (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
    "      )\n",
    "      (dropout1): Dropout(p=0.0, inplace=False)\n",
    "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
    "      (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
    "      (dropout2): Dropout(p=0.0, inplace=False)\n",
    "      (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
    "      (dropout3): Dropout(p=0.0, inplace=False)\n",
    "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
    "    )\n",
    "  (text_layers): ModuleList(\n",
    "    (0-5): TransformerEncoderLayer(\n",
    "      (self_attn): MultiheadAttention(\n",
    "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
    "      )\n",
    "      (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
    "      (dropout): Dropout(p=0.0, inplace=False)\n",
    "      (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
    "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
    "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
    "      (dropout1): Dropout(p=0.0, inplace=False)\n",
    "      (dropout2): Dropout(p=0.0, inplace=False)\n",
    "    )\n",
    "  (fusion_layers): ModuleList(\n",
    "    (0-5): BiAttentionBlock(\n",
    "      (layer_norm_v): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
    "      (layer_norm_l): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
    "      (attn): BiMultiHeadAttention(\n",
    "        (v_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
    "        (l_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
    "        (values_v_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
    "        (values_l_proj): Linear(in_features=256, out_features=1024, bias=True)\n",
    "        (out_v_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
    "        (out_l_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
    "      )\n",
    "      (drop_path): DropPath()\n",
    "    )\n",
    "\n",
    "(Pdb) model.transformer.decoder\n",
    "TransformerDecoder(\n",
    "  (layers): ModuleList(\n",
    "    (0-5): DeformableTransformerDecoderLayer(\n",
    "      (cross_attn): MultiScaleDeformableAttention(\n",
    "        (sampling_offsets): Linear(in_features=256, out_features=256, bias=True)\n",
    "        (attention_weights): Linear(in_features=256, out_features=128, bias=True)\n",
    "        (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
    "        (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
    "        )\n",
    "      (dropout1): Identity()\n",
    "      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
    "      (ca_text): MultiheadAttention(\n",
    "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True))\n",
    "      (catext_dropout): Identity()\n",
    "      (catext_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
    "      (self_attn): MultiheadAttention(\n",
    "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True))\n",
    "      (dropout2): Identity()\n",
    "      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
    "      (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
    "      (dropout3): Identity()\n",
    "      (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
    "      (dropout4): Identity()\n",
    "      (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
    "    )\n",
    "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
    "  (ref_point_head): MLP(\n",
    "    (layers): ModuleList(\n",
    "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
    "      (1): Linear(in_features=256, out_features=256, bias=True))\n",
    "    )\n",
    "  (bbox_embed): ModuleList(\n",
    "    (0-5): MLP(\n",
    "      (layers): ModuleList(\n",
    "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
    "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
    "        (2): Linear(in_features=256, out_features=4, bias=True))\n",
    "    )\n",
    "\n",
    "###\n",
    "(Pdb) model.bert\n",
    "BertModelWarper(\n",
    "  (embeddings): BertEmbeddings(\n",
    "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
    "    (position_embeddings): Embedding(512, 768)\n",
    "    (token_type_embeddings): Embedding(2, 768)\n",
    "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "    (dropout): Dropout(p=0.1, inplace=False)\n",
    "  )\n",
    "  (encoder): BertEncoder(\n",
    "    (layer): ModuleList(\n",
    "      (0-11): BertLayer(\n",
    "        (attention): BertAttention(\n",
    "          (self): BertSelfAttention(\n",
    "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False))\n",
    "          (output): BertSelfOutput(\n",
    "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
    "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "            (dropout): Dropout(p=0.1, inplace=False))\n",
    "        )\n",
    "        (intermediate): BertIntermediate(\n",
    "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
    "          (intermediate_act_fn): GELUActivation()\n",
    "        )\n",
    "        (output): BertOutput(\n",
    "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
    "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "\n",
    "### \n",
    "(Pdb) model.backbone\n",
    "Joiner(\n",
    "  (0-1): SwinTransformer(\n",
    "    (patch_embed): PatchEmbed(\n",
    "      (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
    "      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True))\n",
    "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
    "    (layers): ModuleList(\n",
    "      (0-3): BasicLayer(\n",
    "        (blocks): ModuleList(\n",
    "          (0-?): SwinTransformerBlock(\n",
    "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
    "            (attn): WindowAttention(\n",
    "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
    "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
    "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
    "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
    "              (softmax): Softmax(dim=-1))\n",
    "            (drop_path): Identity()\n",
    "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
    "            (mlp): Mlp(\n",
    "              (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
    "              (act): GELU(approximate=none)\n",
    "              (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
    "              (drop): Dropout(p=0.0, inplace=False))\n",
    "          )\n",
    "    (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
    "    (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
    "    (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "   )\n",
    "  (1): PositionEmbeddingSineHW()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
